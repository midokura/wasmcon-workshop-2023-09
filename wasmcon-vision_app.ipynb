{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision and Sensing Application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to run a Vision and Sensing App in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONUNBUFFERED'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.dup2(0, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_WORK_SPACE = 'samples/detection-single'\n",
    "IP_ADDR='192.168.1.61'\n",
    "\n",
    "! wedge-cli config set webserver.host=$IP_ADDR\n",
    "! cd $APP_WORK_SPACE && wedge-cli build arm64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "MODEL_URL=f\"https://tonibc.blob.core.windows.net/vsa/face_detection_mobilenet_v2_ssd_lite_fpn_quant.tflite?sp=r&st=2023-07-16T21:23:44Z&se=2024-10-01T05:23:44Z&spr=https&sv=2022-11-02&sr=b&sig=q8rvTt9Kh3rNmWDLje%2BMtk8E1%2FXVuxM1Nh2lOq%2F8ctE%3D\"\n",
    "\n",
    "with open(file=os.path.join(r'/assets', 'detection.tflite'), mode=\"wb\") as sample_blob:\n",
    "    download_stream = urlopen(MODEL_URL)\n",
    "    sample_blob.write(download_stream.read())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the model, image and the Vision App in `./assets` folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "IMAGE = \"./assets/face.png\"\n",
    "image = cv2.cvtColor(cv2.imread(IMAGE), cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, dsize=(300, 300))\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up EVP and SensCord to enumate Cloud app and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evp_mock.mock import evp\n",
    "from senscord_mock.mock import senscord\n",
    "from nn_mock.tflite import wasi_nn\n",
    "\n",
    "MODEL=\"./assets/detect.tflite\"\n",
    "\n",
    "# Instantiate the mock EVP and SensCord\n",
    "evp_mock = evp.MockEVP()\n",
    "sensor_mock = senscord.MockSenscord()\n",
    "wasi_nn_mock = wasi_nn.WASI_NN(MODEL)\n",
    "sensor_mock.set_input(image)\n",
    "wasi_nn_mock.set_input(np.expand_dims(image.astype(np.float32) / 255.0, axis=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put commands in event queue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# put the shut down command in queue\n",
    "\n",
    "\n",
    "stream = \"raspicam_image_stream.0\"\n",
    "\n",
    "# send a message to the wasm app\n",
    "p_param_str = f\"\"\"{{\"stream\\\":\\\"{stream}\\\",\n",
    "            \\\"model\\\":\\\"{MODEL_URL}\\\"}}\n",
    "\"\"\"\n",
    "# put in queue\n",
    "#p_param_str = bytes(config, encoding=\"utf-8\")\n",
    "e = evp.EVPRpc(method=\"config\", params=p_param_str)\n",
    "evp_mock.injectEvent(e)\n",
    "\n",
    "e = evp.EVPShutdown()\n",
    "evp_mock.injectEvent(e)  \n",
    "\n",
    "e = evp.EVPShutdown()\n",
    "evp_mock.injectEvent(e)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Wasm Vision and Sensing Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from node.api import Node\n",
    "APP_WORK_SPACE = 'samples/detection-single'\n",
    "VISION_APP = APP_WORK_SPACE + '/bin/node.wasm'\n",
    "\n",
    " \n",
    "# start the wasm app \n",
    "node = Node()\n",
    "node.register_natives(evp_mock, sensor_mock)\n",
    "node.register_nn(wasi_nn_mock)\n",
    "\n",
    "node.load_module(VISION_APP)\n",
    "# if you want to debug the wasm app, uncomment the following line\n",
    "\n",
    "#node.start_debugging()\n",
    "node.main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy AI model and wasm VSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd $APP_WORK_SPACE && wedge-cli deploy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e6a8208b5c026a8c0de9425c389826232a6a2d0a5827555e10540e38d8b94bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
